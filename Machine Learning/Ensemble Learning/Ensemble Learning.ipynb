{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Theoretical Questions\n",
    "\n",
    "1. **Can we use Bagging for regression problems?**  \n",
    "   Yes. Bagging (Bootstrap Aggregating) isn’t limited to classification; it can be applied to regression by training multiple regression models (e.g., Decision Tree Regressors) on different bootstrap samples and averaging their predictions. This reduces variance and improves stability.\n",
    "\n",
    "2. **What is the difference between multiple model training and single model training?**  \n",
    "   Single model training involves building one model on the entire dataset, which may suffer from high variance or bias. In contrast, multiple model training (ensemble learning) builds several models on varied subsets or with different initializations. Their aggregated outputs (by voting or averaging) usually lead to better generalization and robustness.\n",
    "\n",
    "3. **Explain the concept of feature randomness in Random Forest.**  \n",
    "   Random Forest introduces feature randomness by selecting a random subset of features at each split in every decision tree. This ensures that the trees are less correlated, increases diversity in the ensemble, and reduces overfitting, thereby improving the model’s predictive performance.\n",
    "\n",
    "4. **What is OOB (Out-of-Bag) Score?**  \n",
    "   When using bootstrap sampling, not all samples are selected for training each individual model. The left-out samples (out-of-bag samples) can be used to estimate model performance. The OOB score is an internal validation metric that provides an unbiased performance estimate without needing a separate validation set.\n",
    "\n",
    "5. **How can you measure the importance of features in a Random Forest model?**  \n",
    "   Feature importance in Random Forest can be assessed by:\n",
    "   - **Gini Importance (Mean Decrease in Impurity):** Measures the average reduction in impurity from splits using that feature.\n",
    "   - **Permutation Importance:** Evaluates the decrease in model performance when a feature’s values are randomly shuffled.\n",
    "   These methods help identify the most influential features in the dataset.\n",
    "\n",
    "6. **Explain the working principle of a Bagging Classifier.**  \n",
    "   A Bagging Classifier trains multiple base classifiers (often decision trees) on different bootstrap samples. Each classifier votes for a class, and the final prediction is determined by majority vote. This ensemble approach reduces variance and helps prevent overfitting.\n",
    "\n",
    "7. **How do you evaluate a Bagging Classifier’s performance?**  \n",
    "   You can assess performance using:\n",
    "   - **Cross-validation:** Accuracy, precision, recall, F1-score, ROC-AUC, etc.\n",
    "   - **Out-of-Bag Estimation:** Uses OOB samples for performance estimation.\n",
    "   - **Test Set Evaluation:** Comparing predictions against true labels on a hold-out set.\n",
    "\n",
    "8. **How does a Bagging Regressor work?**  \n",
    "   Similar to the classifier version, a Bagging Regressor trains multiple base regressors on bootstrap samples. It then averages their predictions to produce a final regression output, thus reducing prediction variance and improving robustness.\n",
    "\n",
    "9. **What is the main advantage of ensemble techniques?**  \n",
    "   Ensemble techniques improve overall predictive performance by combining multiple models to reduce variance and bias. This aggregation typically results in better generalization and increased stability compared to single models.\n",
    "\n",
    "10. **What is the main challenge of ensemble methods?**  \n",
    "    The main challenges include increased computational cost, decreased interpretability, and the need for careful tuning. The complexity of combining several models and ensuring sufficient diversity among them can also be challenging.\n",
    "\n",
    "11. **Explain the key idea behind ensemble techniques.**  \n",
    "    The key idea is to combine several weak or moderately strong learners in such a way that their collective decision minimizes errors. By averaging or voting on predictions, ensembles reduce the impact of individual model mistakes and yield a more robust overall model.\n",
    "\n",
    "12. **What is a Random Forest Classifier?**  \n",
    "    A Random Forest Classifier is an ensemble of decision trees built using bootstrap samples and random feature selection. Each tree casts a vote for the final class prediction, and the aggregated votes produce a robust and accurate model.\n",
    "\n",
    "13. **What are the main types of ensemble techniques?**  \n",
    "    The main types include:\n",
    "    - **Bagging:** Reduces variance through parallel training on bootstrap samples.\n",
    "    - **Boosting:** Sequentially trains models, with each new model focusing on correcting the errors of its predecessor.\n",
    "    - **Stacking:** Combines predictions from several base models using a meta-model to make final predictions.\n",
    "\n",
    "14. **What is ensemble learning in machine learning?**  \n",
    "    Ensemble learning refers to combining multiple models to solve a problem, leveraging the strengths of each individual model. The aggregated result usually performs better than any single model, providing improved accuracy and robustness.\n",
    "\n",
    "15. **When should we avoid using ensemble methods?**  \n",
    "    Avoid ensemble methods when:\n",
    "    - Model interpretability is critical.\n",
    "    - Computational resources are limited.\n",
    "    - The dataset is very small, potentially leading to overfitting.\n",
    "    - A single model already provides high accuracy with simpler implementation.\n",
    "\n",
    "16. **How does Bagging help in reducing overfitting?**  \n",
    "    By training several models on different subsets of data and averaging their outputs, bagging minimizes the effect of outliers and noisy data. This process reduces variance and makes the final model less likely to overfit compared to a single model.\n",
    "\n",
    "17. **Why is Random Forest better than a single Decision Tree?**  \n",
    "    Random Forest aggregates many decision trees, which lowers the variance and reduces overfitting. Feature randomness and the ensemble voting mechanism make it more robust and accurate than a single decision tree, which is prone to high variance.\n",
    "\n",
    "18. **What is the role of bootstrap sampling in Bagging?**  \n",
    "    Bootstrap sampling creates multiple varied datasets by randomly sampling (with replacement) from the original data. Each base learner is trained on a different sample, ensuring diversity among the models and reducing overall variance when their predictions are aggregated.\n",
    "\n",
    "19. **What are some real-world applications of ensemble techniques?**  \n",
    "    Applications include:\n",
    "    - **Finance:** Fraud detection, risk analysis.\n",
    "    - **Healthcare:** Disease diagnosis, patient outcome predictions.\n",
    "    - **Computer Vision:** Image classification, object detection.\n",
    "    - **NLP:** Sentiment analysis, spam filtering.\n",
    "    - **Recommendation Systems:** Personalized recommendations.\n",
    "\n",
    "20. **What is the difference between Bagging and Boosting?**  \n",
    "    - **Bagging:** Builds multiple models in parallel on bootstrap samples and aggregates predictions (e.g., majority vote or averaging) to reduce variance.\n",
    "    - **Boosting:** Trains models sequentially, with each new model focusing on the errors of the previous ones, thereby reducing bias.\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Practical Exercises in Python (Theoretical Tasks)\n",
    "\n",
    "Below are sample Python code snippets using scikit-learn to implement and evaluate various ensemble methods.\n",
    "\n",
    "### 2.1. Bagging Classifier with Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize Bagging Classifier with Decision Tree as the base estimator\n",
    "bagging_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = bagging_clf.predict(X_test)\n",
    "print(\"Bagging Classifier Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Bagging Regressor with Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Regressor MSE: 3237.526541353384\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes = load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize Bagging Regressor\n",
    "bagging_reg = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=10, random_state=42)\n",
    "\n",
    "# Train and predict\n",
    "bagging_reg.fit(X_train, y_train)\n",
    "y_pred = bagging_reg.predict(X_test)\n",
    "\n",
    "# Evaluate using Mean Squared Error\n",
    "print(\"Bagging Regressor MSE:\", mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Random Forest Classifier on Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean radius: 0.0348\n",
      "mean texture: 0.0152\n",
      "mean perimeter: 0.0680\n",
      "mean area: 0.0605\n",
      "mean smoothness: 0.0080\n",
      "mean compactness: 0.0116\n",
      "mean concavity: 0.0669\n",
      "mean concave points: 0.1070\n",
      "mean symmetry: 0.0034\n",
      "mean fractal dimension: 0.0026\n",
      "radius error: 0.0143\n",
      "texture error: 0.0037\n",
      "perimeter error: 0.0101\n",
      "area error: 0.0296\n",
      "smoothness error: 0.0047\n",
      "compactness error: 0.0056\n",
      "concavity error: 0.0058\n",
      "concave points error: 0.0038\n",
      "symmetry error: 0.0035\n",
      "fractal dimension error: 0.0059\n",
      "worst radius: 0.0828\n",
      "worst texture: 0.0175\n",
      "worst perimeter: 0.0808\n",
      "worst area: 0.1394\n",
      "worst smoothness: 0.0122\n",
      "worst compactness: 0.0199\n",
      "worst concavity: 0.0373\n",
      "worst concave points: 0.1322\n",
      "worst symmetry: 0.0082\n",
      "worst fractal dimension: 0.0045\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load dataset\n",
    "cancer = load_breast_cancer()\n",
    "X, y = cancer.data, cancer.target\n",
    "\n",
    "# Train Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X, y)\n",
    "\n",
    "# Print feature importance scores\n",
    "importances = rf_clf.feature_importances_\n",
    "for name, imp in zip(cancer.feature_names, importances):\n",
    "    print(f\"{name}: {imp:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Compare Random Forest Regressor vs. Single Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regressor MSE: 2859.641982706767\n",
      "Decision Tree Regressor MSE: 5697.789473684211\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Train Random Forest Regressor\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_reg.fit(X_train, y_train)\n",
    "rf_pred = rf_reg.predict(X_test)\n",
    "rf_mse = mean_squared_error(y_test, rf_pred)\n",
    "\n",
    "# Train Single Decision Tree Regressor\n",
    "dt_reg = DecisionTreeRegressor(random_state=42)\n",
    "dt_reg.fit(X_train, y_train)\n",
    "dt_pred = dt_reg.predict(X_test)\n",
    "dt_mse = mean_squared_error(y_test, dt_pred)\n",
    "\n",
    "print(\"Random Forest Regressor MSE:\", rf_mse)\n",
    "print(\"Decision Tree Regressor MSE:\", dt_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB Score: 0.961335676625659\n"
     ]
    }
   ],
   "source": [
    "# Train a Random Forest Classifier with OOB score enabled\n",
    "rf_clf_oob = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
    "rf_clf_oob.fit(X, y)\n",
    "print(\"OOB Score:\", rf_clf_oob.oob_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Bagging Classifier with SVM as Base Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier (SVM) Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load sample dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Wrap SVC with CalibratedClassifierCV using 'estimator' parameter\n",
    "svm_estimator = SVC(probability=True, random_state=42)\n",
    "calibrated_svm = CalibratedClassifierCV(estimator=svm_estimator, cv=3)\n",
    "\n",
    "# Use the calibrated SVC in BaggingClassifier\n",
    "bagging_svm = BaggingClassifier(estimator=calibrated_svm, n_estimators=10, random_state=42)\n",
    "bagging_svm.fit(X_train, y_train)\n",
    "y_pred_svm = bagging_svm.predict(X_test)\n",
    "print(\"Bagging Classifier (SVM) Accuracy:\", accuracy_score(y_test, y_pred_svm))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7. Compare Random Forest Accuracy with Different Numbers of Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with 10 trees Accuracy: 1.0000\n",
      "Random Forest with 50 trees Accuracy: 1.0000\n",
      "Random Forest with 100 trees Accuracy: 1.0000\n",
      "Random Forest with 200 trees Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "n_estimators_options = [10, 50, 100, 200]\n",
    "for n in n_estimators_options:\n",
    "    rf = RandomForestClassifier(n_estimators=n, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    acc = accuracy_score(y_test, rf.predict(X_test))\n",
    "    print(f\"Random Forest with {n} trees Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8. Bagging Classifier with Logistic Regression and AUC Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier (Logistic Regression) AUC Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Initialize Logistic Regression base estimator\n",
    "lr_estimator = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Create Bagging Classifier using Logistic Regression\n",
    "bagging_lr = BaggingClassifier(estimator=lr_estimator, n_estimators=10, random_state=42)\n",
    "bagging_lr.fit(X_train, y_train)\n",
    "\n",
    "# Use predict_proba to get probabilities for all classes\n",
    "y_probs = bagging_lr.predict_proba(X_test)\n",
    "# Compute ROC AUC Score for multi-class classification using one-vs-rest strategy\n",
    "auc_score = roc_auc_score(y_test, y_probs, multi_class='ovr')\n",
    "print(\"Bagging Classifier (Logistic Regression) AUC Score:\", auc_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9. Random Forest Regressor Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age: 0.0586\n",
      "sex: 0.0111\n",
      "bmi: 0.4000\n",
      "bp: 0.1048\n",
      "s1: 0.0492\n",
      "s2: 0.0471\n",
      "s3: 0.0617\n",
      "s4: 0.0294\n",
      "s5: 0.1666\n",
      "s6: 0.0714\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes = load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the Random Forest Regressor\n",
    "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve feature importances\n",
    "importances_reg = rf_regressor.feature_importances_\n",
    "for name, imp in zip(diabetes.feature_names, importances_reg):\n",
    "    print(f\"{name}: {imp:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10. Compare Ensemble Models: Bagging vs. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier Accuracy: 0.0\n",
      "Random Forest Classifier Accuracy: 0.007518796992481203\n"
     ]
    }
   ],
   "source": [
    "# Bagging Classifier\n",
    "bagging_model = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\n",
    "bagging_model.fit(X_train, y_train)\n",
    "bagging_acc = accuracy_score(y_test, bagging_model.predict(X_test))\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_acc = accuracy_score(y_test, rf_model.predict(X_test))\n",
    "\n",
    "print(\"Bagging Classifier Accuracy:\", bagging_acc)\n",
    "print(\"Random Forest Classifier Accuracy:\", rf_acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
