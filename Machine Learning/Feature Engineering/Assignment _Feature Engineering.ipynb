{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1. What is a parameter?\n",
    "\n",
    "A **parameter** in the context of machine learning is a value that a model learns from the training data. For example, in a linear regression model, the slope (weight) and the intercept (bias) are parameters. They are not set by the user; rather, they are adjusted by the learning algorithm to best fit the data.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. What is correlation?\n",
    "\n",
    "**Correlation** is a statistical measure that describes the degree to which two variables move in relation to each other. It is usually quantified by the correlation coefficient, which ranges from –1 to 1:\n",
    "- A value close to 1 indicates a strong positive relationship.\n",
    "- A value close to –1 indicates a strong negative relationship.\n",
    "- A value near 0 suggests little to no linear relationship.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. What does negative correlation mean?\n",
    "\n",
    "**Negative correlation** means that as one variable increases, the other variable tends to decrease. For instance, if you observe a negative correlation between the amount of exercise and body weight, it implies that higher levels of exercise are generally associated with lower body weight.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Define Machine Learning. What are the main components in Machine Learning?\n",
    "\n",
    "**Machine Learning (ML)** is a field of artificial intelligence where algorithms learn patterns from data to make decisions or predictions without being explicitly programmed for specific tasks. The main components include:\n",
    "- **Data:** The raw input from which the model learns.\n",
    "- **Features:** The measurable properties or characteristics derived from the data.\n",
    "- **Model/Algorithm:** The mathematical structure or method used to learn patterns.\n",
    "- **Training Process:** The method by which the model adjusts its parameters using training data.\n",
    "- **Evaluation:** Techniques (such as using a validation or test set) to measure how well the model performs on unseen data.\n",
    "- **Optimization:** Methods (like gradient descent) used to minimize the error or loss during training.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. What are continuous and categorical variables?\n",
    "\n",
    "- **Continuous variables:** These are numerical variables that can take an infinite number of values within a range. Examples include height, weight, and temperature.\n",
    "- **Categorical variables:** These represent distinct categories or groups. They are qualitative and may be nominal (without an inherent order, e.g., colors or types of fruit) or ordinal (with a specific order, e.g., ratings like “low,” “medium,” “high”).\n",
    "\n",
    "---\n",
    "\n",
    "### 6. What do you mean by training and testing a dataset?\n",
    "\n",
    "**Training and testing a dataset** refer to splitting the available data into two (or sometimes more) parts:\n",
    "- **Training set:** Used by the algorithm to learn or fit the model.\n",
    "- **Test set:** Held out from training and used to evaluate the model’s performance on unseen data. This helps assess how well the model generalizes.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. What is a Test set?\n",
    "\n",
    "A **Test set** is a subset of the dataset that is not used during the training process. Its purpose is to provide an unbiased evaluation of the final model fit on the training dataset. This separation helps in assessing the model’s performance on new, unseen data.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. How do we split data for model fitting (training and testing) in Python?\n",
    "\n",
    "In Python, especially when using the scikit-learn library, you typically use the `train_test_split` function from the `sklearn.model_selection` module. For example:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assume X contains features and y contains labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "\n",
    "This code splits the data so that 20% is reserved for testing and the rest for training.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. How do you approach a Machine Learning problem?\n",
    "\n",
    "A systematic approach to a machine learning problem usually involves:\n",
    "1. **Problem Definition:** Understand and define the problem clearly.\n",
    "2. **Data Collection:** Gather the relevant data.\n",
    "3. **Data Cleaning & Preprocessing:** Handle missing values, outliers, and errors.\n",
    "4. **Exploratory Data Analysis (EDA):** Explore the data to understand its structure, patterns, and relationships.\n",
    "5. **Feature Engineering:** Select, create, and transform features to improve model performance.\n",
    "6. **Model Selection:** Choose an appropriate algorithm or set of algorithms.\n",
    "7. **Training:** Train the model using the training data.\n",
    "8. **Evaluation:** Assess the model on validation/test sets.\n",
    "9. **Hyperparameter Tuning:** Adjust model settings for optimal performance.\n",
    "10. **Deployment:** Implement the model into a production environment.\n",
    "11. **Monitoring & Maintenance:** Continuously monitor the model’s performance over time.\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Why do we have to perform EDA before fitting a model to the data?\n",
    "\n",
    "**Exploratory Data Analysis (EDA)** is crucial because:\n",
    "- It helps in understanding the underlying patterns, distributions, and anomalies in the data.\n",
    "- It guides the selection of appropriate preprocessing methods and model types.\n",
    "- It uncovers potential issues such as missing values, outliers, or skewed distributions.\n",
    "- It allows for the visualization of relationships (like correlation) which can inform feature selection.\n",
    "Performing EDA ensures that you have a good grasp of the data, which ultimately leads to more robust model development.\n",
    "\n",
    "---\n",
    "\n",
    "### 11. What does negative correlation mean? *(Revisited)*\n",
    "\n",
    "As noted earlier, **negative correlation** indicates an inverse relationship between two variables. When one variable increases, the other tends to decrease. This is quantified by a negative correlation coefficient (e.g., –0.8).\n",
    "\n",
    "---\n",
    "\n",
    "### 12. What is causation? Explain the difference between correlation and causation with an example.\n",
    "\n",
    "- **Causation** means that one event is the result of the occurrence of another event; there is a cause-and-effect relationship.\n",
    "- **Correlation** simply indicates that two variables move together, but it does not prove that one causes the other.\n",
    "\n",
    "**Example:**  \n",
    "There might be a high correlation between the number of people who drown and ice cream sales during summer. However, this does not mean that buying ice cream causes drowning. Instead, a lurking variable—hot weather—causes both an increase in ice cream sales and more swimming (leading to a higher risk of drowning).\n",
    "\n",
    "---\n",
    "\n",
    "### 13. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
    "\n",
    "An **optimizer** is an algorithm that adjusts the parameters of a model (such as weights in neural networks) to minimize the loss function during training. Common types include:\n",
    "- **Gradient Descent:** Iteratively updates parameters in the direction of the negative gradient of the loss function.\n",
    "  - *Example:* Standard batch gradient descent updates parameters after calculating the gradient over the entire dataset.\n",
    "- **Stochastic Gradient Descent (SGD):** Updates parameters using one data point at a time, which can be faster and introduce noise that may help escape local minima.\n",
    "- **Mini-batch Gradient Descent:** A compromise between batch and stochastic methods; updates parameters using a small batch of data.\n",
    "- **Adam (Adaptive Moment Estimation):** Combines the benefits of AdaGrad and RMSProp; computes adaptive learning rates for each parameter.\n",
    "  - *Example:* Adam is widely used in training deep neural networks because it converges quickly.\n",
    "- **RMSprop:** An adaptive learning rate method that divides the learning rate for a weight by a running average of recent magnitudes of the gradients for that weight.\n",
    "\n",
    "Each optimizer has its advantages and trade-offs regarding speed, convergence, and stability.\n",
    "\n",
    "---\n",
    "\n",
    "### 14. What does model.fit() do? What arguments must be given?\n",
    "\n",
    "The **model.fit()** method is used to train a machine learning model. It adjusts the model’s parameters to minimize the loss function based on the training data. In scikit-learn, you typically pass:\n",
    "- **X:** The input features.\n",
    "- **y:** The target variable.\n",
    "  \n",
    "For example, in scikit-learn:\n",
    "\n",
    "```python\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "In deep learning frameworks (e.g., Keras), additional parameters like the number of epochs and batch size are often required.\n",
    "\n",
    "---\n",
    "\n",
    "### 15. What does model.predict() do? What arguments must be given?\n",
    "\n",
    "The **model.predict()** method is used to generate predictions from the trained model on new, unseen data. In most libraries, you simply pass the input features for which you want predictions. For example, in scikit-learn:\n",
    "\n",
    "```python\n",
    "predictions = model.predict(X_test)\n",
    "```\n",
    "\n",
    "Optionally, some frameworks allow extra arguments (like batch size in Keras), but at its core, you provide the new input data.\n",
    "\n",
    "---\n",
    "\n",
    "### 16. What are continuous and categorical variables? *(Revisited)*\n",
    "\n",
    "- **Continuous variables:** Numeric values that can take any value within a range (e.g., temperature, salary).\n",
    "- **Categorical variables:** Variables that represent categories or groups (e.g., gender, color, type of car). They can be nominal (no order) or ordinal (with a specific order).\n",
    "\n",
    "---\n",
    "\n",
    "### 17. What is feature scaling? How does it help in Machine Learning? How do we perform scaling in Python?\n",
    "\n",
    "**Feature scaling** is the process of standardizing the range of independent variables or features of data. It helps by:\n",
    "- Ensuring that no single feature dominates others because of its scale.\n",
    "- Helping gradient descent converge more quickly.\n",
    "- Improving the performance of distance-based algorithms (e.g., k-nearest neighbors, clustering).\n",
    "\n",
    "Common methods include:\n",
    "- **Standardization:** Rescales data to have a mean of 0 and a standard deviation of 1. (Using `StandardScaler` in scikit-learn)\n",
    "- **Normalization (Min-Max Scaling):** Rescales the data to a fixed range, usually [0, 1]. (Using `MinMaxScaler` in scikit-learn)\n",
    "\n",
    "Example in Python:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 18. How do we split data for model fitting (training and testing) in Python? *(Revisited)*\n",
    "\n",
    "As mentioned earlier, you can use the `train_test_split` function from scikit-learn:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "\n",
    "This function randomly splits the dataset into training and test sets based on the specified `test_size` (here, 20% for testing).\n",
    "\n",
    "---\n",
    "\n",
    "### 19. What is sklearn.preprocessing?\n",
    "\n",
    "`sklearn.preprocessing` is a module in scikit-learn that provides functions and classes to preprocess data. It includes tools for:\n",
    "- **Scaling and Normalization:** StandardScaler, MinMaxScaler, etc.\n",
    "- **Encoding Categorical Features:** LabelEncoder, OneHotEncoder.\n",
    "- **Binarization, Imputation, and more.**\n",
    "\n",
    "These preprocessing steps are critical to prepare the raw data into a format that can be effectively used by machine learning algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "### 20. What is sklearn.linear_model?\n",
    "\n",
    "`sklearn.linear_model` is a module in scikit-learn that contains implementations of various linear models. It includes algorithms such as:\n",
    "- **LinearRegression:** For predicting continuous outcomes.\n",
    "- **LogisticRegression:** For binary and multi-class classification.\n",
    "- **Ridge, Lasso, and ElasticNet:** For regularized regression techniques.\n",
    "  \n",
    "These models are widely used for problems where the relationship between the independent variables and the target variable is assumed to be linear.\n",
    "\n",
    "---\n",
    "\n",
    "### 21. How can you find correlation between variables in Python?\n",
    "\n",
    "You can find the correlation between variables using:\n",
    "- **Pandas DataFrame method:** `DataFrame.corr()` computes the pairwise correlation of columns.\n",
    "- **Visualization:** Libraries like Seaborn (with `sns.heatmap`) or Matplotlib can be used to visualize the correlation matrix.\n",
    "  \n",
    "Example with Pandas:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "correlation_matrix = df.corr()\n",
    "print(correlation_matrix)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 22. What is correlation? *(Revisited)*\n",
    "\n",
    "As discussed, **correlation** measures the linear relationship between two variables. It is expressed via the correlation coefficient, which ranges between –1 and 1. A value of 1 denotes a perfect positive correlation, –1 a perfect negative correlation, and 0 no correlation.\n",
    "\n",
    "---\n",
    "\n",
    "### 23. What is sklearn.preprocessing? *(Revisited)*\n",
    "\n",
    "This question repeats an earlier one. **`sklearn.preprocessing`** is a scikit-learn module for transforming and scaling data before model training. It includes tools for standardization, normalization, encoding categorical variables, and more.\n",
    "\n",
    "---\n",
    "\n",
    "### 24. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
    "\n",
    "Categorical variables can be handled using several encoding techniques:\n",
    "- **Label Encoding:** Assigns a unique integer to each category (suitable for ordinal data).\n",
    "- **One-Hot Encoding:** Creates binary columns for each category, where each column represents the presence (1) or absence (0) of the category (suitable for nominal data).\n",
    "- **Ordinal Encoding:** Maps categories to ordered integers if there is an intrinsic order.\n",
    "- **Binary Encoding:** Useful when there are many categories; it converts the integer labels into binary digits.\n",
    "  \n",
    "These techniques help convert categorical data into numerical format so that machine learning algorithms can process them.\n",
    "\n",
    "---\n",
    "\n",
    "### 25. How does loss value help in determining whether the model is good or not?\n",
    "\n",
    "The **loss value** (or cost) is a numerical measure of the difference between the predicted outputs and the actual target values. It is used to:\n",
    "- **Evaluate performance:** A lower loss generally indicates that the model is making predictions closer to the true values.\n",
    "- **Guide optimization:** The loss is minimized during training via an optimizer, leading to improved performance.\n",
    "  \n",
    "However, the absolute value of loss should be interpreted in the context of the problem and compared with baseline models. A very low loss is desirable, but it must also be validated with metrics on unseen data to ensure the model generalizes well.\n",
    "\n",
    "---\n",
    "\n",
    "### 26. Explain data encoding.\n",
    "\n",
    "**Data encoding** is the process of converting categorical or textual data into numerical format that can be understood by machine learning algorithms. The most common techniques include:\n",
    "- **Label Encoding:** Each unique category is assigned a numerical value.\n",
    "- **One-Hot Encoding:** Creates new binary columns for each category.\n",
    "- **Ordinal Encoding:** Used when categories have an intrinsic order.\n",
    "  \n",
    "Proper encoding is essential because most ML algorithms require numerical input and cannot directly process raw categorical or textual data.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
